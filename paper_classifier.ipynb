{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Preprint Papers from the ArXiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website [arxiv.org](https://arxiv.org) is a popular database for scientific papers in STEM fields. ArXiv has its own classification system consisting of roughly 150 different categories, which are manually added by the authors whenever a new paper is uploaded. A paper can be assigned multiple categories.\n",
    "\n",
    "The goal for this project is to develop a machine learning model which can predict the ArXiv category from a given title and abstract.\n",
    "\n",
    "We start by importing all the packages we will need and setting up a data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os # used for handling files\n",
    "from sklearn.decomposition import PCA # dimension reduction of data\n",
    "import pickle # saving models\n",
    "from pathlib import Path # to get home directory\n",
    "from functools import reduce # used to calculate accuracy of model\n",
    "\n",
    "# local files\n",
    "import arxiv_scraper\n",
    "import cleaner\n",
    "import elmo\n",
    "import onehot\n",
    "\n",
    "print(\"Packages loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set used here has been scraped from the [ArXiv API](https://arxiv.org/help/api) over several days, using the Python scraper `arxiv_scraper.py`. To get a sense for how long the scraping takes, you can uncomment and run the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#arxiv_scraper.cat_scrape(\n",
    "#    max_results_per_cat = 100, # maximum number of papers to download per category (there are ~150 categories)\n",
    "#    file_path = \"arxiv_data\", # name of output file\n",
    "#    batch_size = 100 # size of every batch - lower batch size requires less memory - must be less than 30,000\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, I have downloaded metadata from about a million papers using this scraper (with `max_results_per_cat` = 10000), which can be freely downloaded below. This data set takes up ~1gb of space, however, so I've included many random samples of this data set as well:\n",
    "\n",
    "* `arxiv` contains the main data set\n",
    "* `arxiv_sample_1000` contains 1,000 papers\n",
    "* `arxiv_sample_5000` contains 5,000 papers\n",
    "* `arxiv_sample_10000` contains 10,000 papers\n",
    "* `arxiv_sample_25000` contains 25,000 papers\n",
    "* `arxiv_sample_50000` contains 50,000 papers\n",
    "* `arxiv_sample_100000` contains 100,000 papers\n",
    "* `arxiv_sample_200000` contains 200,000 papers\n",
    "* `arxiv_sample_500000` contains 500,000 papers\n",
    "* `arxiv_sample_750000` contains 750,000 papers\n",
    "\n",
    "Choose your favorite below. Alternatively, of course, you can set it to be the file name of your own scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"arxiv_sample_200000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we specify the folder in which we will store all our data. Change to whatever folder you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = str(Path.home())\n",
    "data_path = os.path.join(home_dir, \"pCloudDrive\", \"public_folder\", \"scholarly_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then do some basic setting up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats.csv is already downloaded.\n",
      "arxiv_val_1hot.csv is already downloaded.\n",
      "arxiv_val_1hot_agg.csv is already downloaded.\n",
      "arxiv_sample_50000.csv is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "# create path directory and download a list of all arXiv categories\n",
    "cleaner.setup(data_path)\n",
    "\n",
    "# download the raw titles and abstracts\n",
    "cleaner.download_papers(file_name, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we store the list of arXiv categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>astro-ph</td>\n",
       "      <td>Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>astro-ph.CO</td>\n",
       "      <td>Cosmology and Nongalactic Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>astro-ph.EP</td>\n",
       "      <td>Earth and Planetary Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>astro-ph.GA</td>\n",
       "      <td>Astrophysics of Galaxies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>astro-ph.HE</td>\n",
       "      <td>High Energy Astrophysical Phenomena</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                             description\n",
       "0     astro-ph                            Astrophysics\n",
       "1  astro-ph.CO  Cosmology and Nongalactic Astrophysics\n",
       "2  astro-ph.EP        Earth and Planetary Astrophysics\n",
       "3  astro-ph.GA                Astrophysics of Galaxies\n",
       "4  astro-ph.HE     High Energy Astrophysical Phenomena"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct category dataframe and array\n",
    "full_path = os.path.join(data_path, \"cats.csv\")\n",
    "cats_df = pd.read_csv(full_path)\n",
    "cats = np.asarray(cats_df['category'].values)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "cats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do some basic cleaning operations on our raw data. We convert strings '\\[cat_1, cat_2\\]' into actual lists \\[cat_1, cat_2\\], make everything lower case, removing punctuation, numbers and whitespace, and dropping NaN rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last text cleaning step is to lemmatise the text, which reduces all words to its base form. For instance, 'eating' is converted into 'eat' and 'better' is converted into 'good'. This usually takes a while to finish, so instead we're simply going to download a lemmatised version of your chosen data set. Alternatively, if you're dealing with your own scraped data set, you can uncomment the marked lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned text...\n",
      "Shape of clean_df: (49183, 2). Here are some of the lemmatised texts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['math.ST', 'cs.CC', 'stat.ML', 'stat.TH']</td>\n",
       "      <td>fast sparse least - square regression with non - asymptotic guarantee in this paper , -PRON- study a fast approximation method for { \\it large - scale high - dimensional } sparse least - square regression problem by exploit the johnson - lindenstrauss ( jl ) transform , which embe a set of high - dimensional vector into a low - dimensional space . in particular , -PRON- propose to apply the jl transform to the data matrix and the target vector and then to solve a sparse least - square problem on the compress datum with a { \\it slightly large regularization parameter}. theoretically , -PRON- establish the optimization error bind of the learn model for two different sparsity - induce regularizer , i.e. , the elastic net and the $ \\ell_$ norm . compare with previous relevant work , -PRON- analysis be { \\it non - asymptotic and exhibit more insight } on the bound , the sample complexity and the regularization . as an illustration , -PRON- also provide an error bind of the { \\it dantzig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cs.SD']</td>\n",
       "      <td>an interesting property of lpcs for sonorant vs fricative discrimination linear prediction ( lp ) technique estimate an optimum all - pole filter of a give order for a frame of speech signal . the coefficient of the all - pole filter , /a(z ) be refer to as lp coefficient ( lpcs ) . the gain of the inverse of the all - pole filter , a(z ) at z = , i.e , at frequency = , a ( ) correspond to the sum of lpcs , which have the property of be low ( high ) than a threshold for the sonorant ( fricative ) . when the inverse - tan of a ( ) , denote as t ( ) , be use a feature and test on the sonorant and fricative frame of the entire timit database , an accuracy of .% be obtain . hence , -PRON- refer to t ( ) as sonorant - fricative discrimination index ( sfdi ) . this property have also be test for -PRON- robustness for additive white noise and on the telephone quality speech of the ntimit database . these result be comparable to , or in some respect , well than the state - of - the - art m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['math.ST', 'stat.TH', '62F15, 60F05 (Primary) 65C60 (Secondary)']</td>\n",
       "      <td>on some asymptotic property and an almost sure approximation of the normalize inverse - gaussian process in this paper , -PRON- present some asymptotic property of the normalize inverse - gaussian process . in particular , when the concentration parameter be large , -PRON- establish an analogue of the empirical functional central limit theorem , the strong law of large number and the glivenko - cantelli theorem for the normalize inverse - gaussian process and -PRON- corresponding quantile process . -PRON- also derive a finite sum - representation that converge almost surely to the ferguson and klass representation of the normalize inverse - gaussian process . this almost sure approximation can be use to simulate efficiently the normalize inverse - gaussian process .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['quant-ph']</td>\n",
       "      <td>road towards fault - tolerant universal quantum computation current experiment be take the first step toward noise - resilient logical qubit . crucially , a quantum computer must not merely store information , but also process -PRON- . a fault - tolerant computational procedure ensure that error do not multiply and spread . this review compare the lead proposal for promote a quantum memory to a quantum processor . -PRON- compare magic state distillation , color code technique and other alternative idea , pay attention to relative resource demand . -PRON- discuss the several no - go result which hold for low - dimensional topological code and outline the potential reward of use high - dimensional quantum ( ldpc ) code in modular architecture .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['astro-ph.SR', 'astro-ph.EP', 'astro-ph.GA']</td>\n",
       "      <td>model mid - infrared molecular emission line from t tauri star -PRON- introduce a new modelling framework call flit to simulate infrared line emission spectra from protoplanetary disc . this paper focus on the mid - ir spectral region between . um to um for t tauri star . the generate spectra contain several ten of thousand of molecular emission line of ho , oh , co , co , hcn , ch , h and a few other molecule , as well as the forbidden atomic emission line of si , sii , siii , siii , feii , neii , neiii , arii and ariii . in contrast to previously publish work , -PRON- do not treat the abundance of the molecule nor the temperature in the disc as free parameter , but use the complex result of detailed d prodimo disc model concern gas and dust temperature structure , and molecular concentration . flit compute the line emission spectra by ray trace in an efficient , fast and reliable way . the result be broadly consistent with r= spitzer / irs observational datum of t tauri star conc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             category  \\\n",
       "0                          ['math.ST', 'cs.CC', 'stat.ML', 'stat.TH']   \n",
       "1                                                           ['cs.SD']   \n",
       "2  ['math.ST', 'stat.TH', '62F15, 60F05 (Primary) 65C60 (Secondary)']   \n",
       "3                                                        ['quant-ph']   \n",
       "4                       ['astro-ph.SR', 'astro-ph.EP', 'astro-ph.GA']   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                clean_text  \n",
       "0  fast sparse least - square regression with non - asymptotic guarantee in this paper , -PRON- study a fast approximation method for { \\it large - scale high - dimensional } sparse least - square regression problem by exploit the johnson - lindenstrauss ( jl ) transform , which embe a set of high - dimensional vector into a low - dimensional space . in particular , -PRON- propose to apply the jl transform to the data matrix and the target vector and then to solve a sparse least - square problem on the compress datum with a { \\it slightly large regularization parameter}. theoretically , -PRON- establish the optimization error bind of the learn model for two different sparsity - induce regularizer , i.e. , the elastic net and the $ \\ell_$ norm . compare with previous relevant work , -PRON- analysis be { \\it non - asymptotic and exhibit more insight } on the bound , the sample complexity and the regularization . as an illustration , -PRON- also provide an error bind of the { \\it dantzig...  \n",
       "1  an interesting property of lpcs for sonorant vs fricative discrimination linear prediction ( lp ) technique estimate an optimum all - pole filter of a give order for a frame of speech signal . the coefficient of the all - pole filter , /a(z ) be refer to as lp coefficient ( lpcs ) . the gain of the inverse of the all - pole filter , a(z ) at z = , i.e , at frequency = , a ( ) correspond to the sum of lpcs , which have the property of be low ( high ) than a threshold for the sonorant ( fricative ) . when the inverse - tan of a ( ) , denote as t ( ) , be use a feature and test on the sonorant and fricative frame of the entire timit database , an accuracy of .% be obtain . hence , -PRON- refer to t ( ) as sonorant - fricative discrimination index ( sfdi ) . this property have also be test for -PRON- robustness for additive white noise and on the telephone quality speech of the ntimit database . these result be comparable to , or in some respect , well than the state - of - the - art m...  \n",
       "2                                                                                                                                                                                                                                 on some asymptotic property and an almost sure approximation of the normalize inverse - gaussian process in this paper , -PRON- present some asymptotic property of the normalize inverse - gaussian process . in particular , when the concentration parameter be large , -PRON- establish an analogue of the empirical functional central limit theorem , the strong law of large number and the glivenko - cantelli theorem for the normalize inverse - gaussian process and -PRON- corresponding quantile process . -PRON- also derive a finite sum - representation that converge almost surely to the ferguson and klass representation of the normalize inverse - gaussian process . this almost sure approximation can be use to simulate efficiently the normalize inverse - gaussian process .  \n",
       "3                                                                                                                                                                                                                                                         road towards fault - tolerant universal quantum computation current experiment be take the first step toward noise - resilient logical qubit . crucially , a quantum computer must not merely store information , but also process -PRON- . a fault - tolerant computational procedure ensure that error do not multiply and spread . this review compare the lead proposal for promote a quantum memory to a quantum processor . -PRON- compare magic state distillation , color code technique and other alternative idea , pay attention to relative resource demand . -PRON- discuss the several no - go result which hold for low - dimensional topological code and outline the potential reward of use high - dimensional quantum ( ldpc ) code in modular architecture .  \n",
       "4  model mid - infrared molecular emission line from t tauri star -PRON- introduce a new modelling framework call flit to simulate infrared line emission spectra from protoplanetary disc . this paper focus on the mid - ir spectral region between . um to um for t tauri star . the generate spectra contain several ten of thousand of molecular emission line of ho , oh , co , co , hcn , ch , h and a few other molecule , as well as the forbidden atomic emission line of si , sii , siii , siii , feii , neii , neiii , arii and ariii . in contrast to previously publish work , -PRON- do not treat the abundance of the molecule nor the temperature in the disc as free parameter , but use the complex result of detailed d prodimo disc model concern gas and dust temperature structure , and molecular concentration . flit compute the line emission spectra by ray trace in an efficient , fast and reliable way . the result be broadly consistent with r= spitzer / irs observational datum of t tauri star conc...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_path = os.path.join(data_path, f\"{file_name}_clean.csv\")\n",
    "if not os.path.isfile(full_path):\n",
    "    # preclean raw data and save the precleaned texts and\n",
    "    # categories to {file_name}_preclean.csv\n",
    "    cleaner.get_preclean_text(file_name, data_path)\n",
    "\n",
    "    # lemmatise precleaned data and save lemmatised texts to \n",
    "    # {file_name}_clean.csv and delete the precleaned file\n",
    "    cleaner.lemmatise_file(file_name, batch_size = 1000, path = data_path, confirmation = False)\n",
    "\n",
    "# load in cleaned text\n",
    "print(\"Loading cleaned text...\")\n",
    "full_path = os.path.join(data_path, f\"{file_name}_clean.csv\")\n",
    "clean_text = pd.read_csv(full_path, header = None)\n",
    "clean_df = pd.DataFrame(clean_text)\n",
    "clean_df.columns = ['category', 'clean_text']\n",
    "\n",
    "print(f\"Shape of clean_df: {clean_df.shape}. Here are some of the lemmatised texts:\")\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then perform a one hot encoding for the category variable, as this will make training our model easier. We do this by first creating a dataframe with columns the categories and binary values for every paper, and then concatenate our original dataframe with the binary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading category data...\n",
      "Category data loaded.\n",
      "Dimensions of df_1hot: (199998, 8).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modulus stable map genus logarithmic geometry pair paper develop framework application logarithm...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>break symmetric cryptosystem quantum period find shor algorithm quantum computer severe threat p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>improve surgical training phantom hyperrealism deep unpaired image image translation real surger...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chaotic dynamic bounce coin study dynamic bounce coin motion restrict dimensional plane coin mod...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>geographica benchmark geospatial rdf store geospatial extension sparql like geosparql stsparql r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     0  \\\n",
       "0  modulus stable map genus logarithmic geometry pair paper develop framework application logarithm...   \n",
       "1  break symmetric cryptosystem quantum period find shor algorithm quantum computer severe threat p...   \n",
       "2  improve surgical training phantom hyperrealism deep unpaired image image translation real surger...   \n",
       "3  chaotic dynamic bounce coin study dynamic bounce coin motion restrict dimensional plane coin mod...   \n",
       "4  geographica benchmark geospatial rdf store geospatial extension sparql like geosparql stsparql r...   \n",
       "\n",
       "   1  2  3  4  5  6  7  \n",
       "0  0  0  0  0  0  0  0  \n",
       "1  0  0  0  0  0  0  0  \n",
       "2  0  0  0  0  0  0  0  \n",
       "3  0  0  0  0  0  0  0  \n",
       "4  0  0  0  0  0  0  0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode categories\n",
    "#onehot.onehot_encode(file_name, data_path)\n",
    "\n",
    "# load data\n",
    "print(\"Loading category data...\")\n",
    "full_path = os.path.join(data_path, f\"{file_name}_1hot_agg.csv\")\n",
    "df_1hot = pd.read_csv(full_path, header = None)\n",
    "print(f\"Category data loaded.\")\n",
    "\n",
    "# show the new columns of the data frame\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "print(f\"Dimensions of df_1hot: {df_1hot.shape}.\")\n",
    "df_1hot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our model we have to extract features from the titles and abstracts. We will be using ELMo, a state-of-the-art NLP framework developed by AllenNLP, which converts text input into vectors, with similar words being closer to each other. We will first download the ELMo model. It is over 350mb in size, so it might take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo model already downloaded.\n"
     ]
    }
   ],
   "source": [
    "elmo.download_elmo_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to extract ELMo features from our cleaned text data. This is done using the `extract` function from `elmo.py`. This usually takes a LONG time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ELMo'd text...\n",
      "ELMo data loaded from arxiv_sample_50000_elmo.csv.\n",
      "Shape of elmo_df: (49999, 1025)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032264</td>\n",
       "      <td>0.164495</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.008434</td>\n",
       "      <td>0.062614</td>\n",
       "      <td>-0.097488</td>\n",
       "      <td>-0.062888</td>\n",
       "      <td>0.252686</td>\n",
       "      <td>0.095110</td>\n",
       "      <td>-0.187274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077523</td>\n",
       "      <td>-0.038856</td>\n",
       "      <td>0.107055</td>\n",
       "      <td>0.255856</td>\n",
       "      <td>-0.061011</td>\n",
       "      <td>0.394257</td>\n",
       "      <td>-0.011740</td>\n",
       "      <td>0.138539</td>\n",
       "      <td>-0.013543</td>\n",
       "      <td>['math...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.180230</td>\n",
       "      <td>0.266668</td>\n",
       "      <td>0.042819</td>\n",
       "      <td>-0.038384</td>\n",
       "      <td>0.038294</td>\n",
       "      <td>-0.034775</td>\n",
       "      <td>-0.023941</td>\n",
       "      <td>0.367830</td>\n",
       "      <td>0.070210</td>\n",
       "      <td>-0.084328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116197</td>\n",
       "      <td>-0.024739</td>\n",
       "      <td>0.109669</td>\n",
       "      <td>0.140959</td>\n",
       "      <td>-0.035091</td>\n",
       "      <td>0.431889</td>\n",
       "      <td>-0.092570</td>\n",
       "      <td>0.226117</td>\n",
       "      <td>-0.007482</td>\n",
       "      <td>['cs.SD']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.016303</td>\n",
       "      <td>0.067849</td>\n",
       "      <td>0.052801</td>\n",
       "      <td>-0.009187</td>\n",
       "      <td>-0.007150</td>\n",
       "      <td>-0.064071</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.117463</td>\n",
       "      <td>-0.027563</td>\n",
       "      <td>-0.178627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057723</td>\n",
       "      <td>0.033624</td>\n",
       "      <td>0.099338</td>\n",
       "      <td>0.163222</td>\n",
       "      <td>0.029108</td>\n",
       "      <td>0.164415</td>\n",
       "      <td>0.038682</td>\n",
       "      <td>0.215892</td>\n",
       "      <td>0.023488</td>\n",
       "      <td>['math...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002490</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>-0.025218</td>\n",
       "      <td>-0.051667</td>\n",
       "      <td>0.082643</td>\n",
       "      <td>-0.084306</td>\n",
       "      <td>-0.021061</td>\n",
       "      <td>0.037081</td>\n",
       "      <td>0.052564</td>\n",
       "      <td>-0.164204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040261</td>\n",
       "      <td>0.004867</td>\n",
       "      <td>0.136856</td>\n",
       "      <td>0.159625</td>\n",
       "      <td>0.058569</td>\n",
       "      <td>0.285389</td>\n",
       "      <td>0.003747</td>\n",
       "      <td>0.125958</td>\n",
       "      <td>-0.007329</td>\n",
       "      <td>['quan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.217903</td>\n",
       "      <td>0.444179</td>\n",
       "      <td>-0.155039</td>\n",
       "      <td>-0.028567</td>\n",
       "      <td>0.113540</td>\n",
       "      <td>-0.132693</td>\n",
       "      <td>-0.209251</td>\n",
       "      <td>0.191103</td>\n",
       "      <td>0.145439</td>\n",
       "      <td>-0.233822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064445</td>\n",
       "      <td>-0.042244</td>\n",
       "      <td>0.139357</td>\n",
       "      <td>0.401823</td>\n",
       "      <td>0.134148</td>\n",
       "      <td>0.919678</td>\n",
       "      <td>-0.068214</td>\n",
       "      <td>0.570734</td>\n",
       "      <td>0.007093</td>\n",
       "      <td>['astr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.032264  0.164495  0.000046 -0.008434  0.062614 -0.097488 -0.062888   \n",
       "1  0.180230  0.266668  0.042819 -0.038384  0.038294 -0.034775 -0.023941   \n",
       "2 -0.016303  0.067849  0.052801 -0.009187 -0.007150 -0.064071  0.004345   \n",
       "3  0.002490  0.063034 -0.025218 -0.051667  0.082643 -0.084306 -0.021061   \n",
       "4  0.217903  0.444179 -0.155039 -0.028567  0.113540 -0.132693 -0.209251   \n",
       "\n",
       "          7         8         9  ...      1015      1016      1017      1018  \\\n",
       "0  0.252686  0.095110 -0.187274  ...  0.077523 -0.038856  0.107055  0.255856   \n",
       "1  0.367830  0.070210 -0.084328  ...  0.116197 -0.024739  0.109669  0.140959   \n",
       "2  0.117463 -0.027563 -0.178627  ...  0.057723  0.033624  0.099338  0.163222   \n",
       "3  0.037081  0.052564 -0.164204  ...  0.040261  0.004867  0.136856  0.159625   \n",
       "4  0.191103  0.145439 -0.233822  ...  0.064445 -0.042244  0.139357  0.401823   \n",
       "\n",
       "       1019      1020      1021      1022      1023   category  \n",
       "0 -0.061011  0.394257 -0.011740  0.138539 -0.013543  ['math...  \n",
       "1 -0.035091  0.431889 -0.092570  0.226117 -0.007482  ['cs.SD']  \n",
       "2  0.029108  0.164415  0.038682  0.215892  0.023488  ['math...  \n",
       "3  0.058569  0.285389  0.003747  0.125958 -0.007329  ['quan...  \n",
       "4  0.134148  0.919678 -0.068214  0.570734  0.007093  ['astr...  \n",
       "\n",
       "[5 rows x 1025 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_path = os.path.join(data_path, f\"{file_name}_elmo.csv\")\n",
    "if not os.path.isfile(full_path):\n",
    "    # extract ELMo data\n",
    "    elmo.extract(\n",
    "        file_name = file_name,\n",
    "        path = data_path,\n",
    "        batch_size = 20, # lower batch size gives less accurate vectors but requires less memory\n",
    "        doomsday_clock = 50,\n",
    "        confirmation = False\n",
    "        )\n",
    "\n",
    "# load ELMo data\n",
    "print(\"Loading ELMo'd text...\")\n",
    "full_path = os.path.join(data_path, f\"{file_name}_elmo.csv\")\n",
    "elmo_data = pd.read_csv(full_path, header = None)\n",
    "print(f\"ELMo data loaded from {file_name}_elmo.csv.\")\n",
    "\n",
    "elmo_df = clean_df.copy()\n",
    "elmo_df = elmo_data.join(elmo_df['category'])\n",
    "\n",
    "print(f\"Shape of elmo_df: {elmo_df.shape}\")\n",
    "elmo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = elmo_df.shape[1] - 1\n",
    "X = np.asarray(elmo_df.iloc[:, :n])\n",
    "X_2d = PCA(n_components = 2).fit_transform(X)\n",
    "elmo_2d = pd.DataFrame(X_2d, columns = ['x', 'y'])\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize = (15, 10))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "sns.scatterplot(data = elmo_2d, x = 'x', y = 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the categories in our data set are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a dataframe with the amount of papers in each category\n",
    "sum_cats = df_1hot.iloc[:, 1024:].apply(lambda x: x.sum())\n",
    "\n",
    "# get statistical information about the distribution of the amount of papers\n",
    "sum_cats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the amount of papers in each category\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.bar(x = sum_cats.keys(), height = sum_cats.values)\n",
    "plt.xlabel('Categories', fontsize = 13)\n",
    "plt.ylabel('Number of papers', fontsize = 13)\n",
    "plt.title('Distribution of categories in data set', fontsize = 18)\n",
    "#plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our data is not particularly uniformly distributed. These are the categories with the most amount of papers in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the counts to the dataframe and sort \n",
    "cats_df['count'] = sum_cats.values\n",
    "cats_df = cats_df.sort_values(by=['count'], ascending = False)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "cats_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now done manipulating our data, and the time has come to build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
