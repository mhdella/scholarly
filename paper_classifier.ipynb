{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Preprint Papers from the ArXiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website [arxiv.org](https://arxiv.org) is a popular database for scientific papers in STEM fields. ArXiv has its own classification system consisting of roughly 150 different categories, which are manually added by the authors whenever a new paper is uploaded. A paper can be assigned multiple categories.\n",
    "\n",
    "The goal for this project is to develop a machine learning model which can predict the ArXiv category from a given title and abstract.\n",
    "\n",
    "We start by importing all the packages we will need and setting up a data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.decomposition import PCA # dimension reduction of data\n",
    "\n",
    "# local files\n",
    "import arxiv_scraper\n",
    "import cleaner\n",
    "import elmo\n",
    "import NN\n",
    "\n",
    "print(\"Packages loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set used here has been scraped from the [ArXiv API](https://arxiv.org/help/api) over several days, using the Python scraper `arxiv_scraper.py`. To get a sense for how long the scraping takes, you can uncomment and run the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#arxiv_scraper.cat_scrape(\n",
    "#    max_results_per_cat = 100, # maximum number of papers to download per category (there are ~150 categories)\n",
    "#    file_path = \"arxiv_data\", # name of output file\n",
    "#    batch_size = 100 # size of every batch - lower batch size requires less memory - must be less than 30,000\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, I have downloaded metadata from about a million papers using this scraper (with `max_results_per_cat` = 10000), which can be freely downloaded below. This data set takes up ~1gb of space, however, so I've included many random samples of this data set as well:\n",
    "\n",
    "* `arxiv` contains the main data set\n",
    "* `arxiv_sample_1000` contains 1,000 papers\n",
    "* `arxiv_sample_5000` contains 5,000 papers\n",
    "* `arxiv_sample_10000` contains 10,000 papers\n",
    "* `arxiv_sample_25000` contains 25,000 papers\n",
    "* `arxiv_sample_50000` contains 50,000 papers\n",
    "* `arxiv_sample_100000` contains 100,000 papers\n",
    "* `arxiv_sample_200000` contains 200,000 papers\n",
    "* `arxiv_sample_500000` contains 500,000 papers\n",
    "* `arxiv_sample_750000` contains 750,000 papers\n",
    "\n",
    "Choose your favorite below. Alternatively, of course, you can set it to be the file name of your own scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"arxiv_sample_100000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we specify the folder in which we will store all our data. Change to whatever folder you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"/home\", \"dn16382\", \"pCloudDrive\", \"Public Folder\", \"scholarly_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then do some basic setting up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create path directory and download a list of all arXiv categories\n",
    "cleaner.setup(data_path)\n",
    "\n",
    "# download the raw titles and abstracts\n",
    "cleaner.download_papers(file_name, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we store the list of arXiv categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# construct category dataframe and array\n",
    "full_path = os.path.join(data_path, \"cats.csv\")\n",
    "cats_df = pd.read_csv(full_path)\n",
    "cats = np.asarray(cats_df['category'].values)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "cats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do some basic cleaning operations on our raw data. We convert strings '\\[cat_1, cat_2\\]' into actual lists \\[cat_1, cat_2\\], make everything lower case, removing punctuation, numbers and whitespace, and dropping NaN rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last text cleaning step is to lemmatise the text, which reduces all words to its base form. For instance, 'eating' is converted into 'eat' and 'better' is converted into 'good'. This usually takes a while to finish, so instead we're simply going to download a lemmatised version of your chosen data set. Alternatively, if you're dealing with your own scraped data set, you can uncomment the marked lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_path = os.path.join(data_path, f\"{file_name}_clean.csv\")\n",
    "if not os.path.isfile(full_path):\n",
    "    # preclean raw data and save the precleaned texts and\n",
    "    # categories to {file_name}_preclean.csv\n",
    "    cleaner.get_preclean_text(file_name, data_path)\n",
    "\n",
    "    # lemmatise precleaned data and save lemmatised texts to \n",
    "    # {file_name}_clean.csv and delete the precleaned file\n",
    "    cleaner.lemmatise_file(file_name, batch_size = 1000, path = data_path, confirmation = False)\n",
    "\n",
    "# load in cleaned text\n",
    "print(\"Loading cleaned text...\")\n",
    "full_path = os.path.join(data_path, f\"{file_name}_clean.csv\")\n",
    "clean_text = pd.read_csv(full_path, delimiter = '\\n', header = None)\n",
    "clean_df = pd.DataFrame(clean_text)\n",
    "clean_df.columns = ['clean_text']\n",
    "\n",
    "# load in cats and add them to df\n",
    "full_path = os.path.join(data_path, f\"{file_name}.csv\")\n",
    "clean_cats_with_path = lambda x: cleaner.clean_cats(x, path = data_path)\n",
    "cleaned_cats = pd.read_csv(full_path, header = None, converters = {0 : clean_cats_with_path})\n",
    "\n",
    "# join the two dataframes\n",
    "clean_df['category'] = cleaned_cats.iloc[:, 0]\n",
    "\n",
    "print(f\"Shape of clean_df: {clean_df.shape}. Here are some of the lemmatised texts:\")\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "clean_df[['clean_text', 'category']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our model we have to extract features from the titles and abstracts. We will be using ELMo, a state-of-the-art NLP framework developed by AllenNLP, which converts text input into vectors, with similar words being closer to each other. We will first download the ELMo model. It is over 350mb in size, so it might take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo.download_elmo_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to extract ELMo features from our cleaned text data. This is done using the `extract` function from `elmo.py`. This usually takes a LONG time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = os.path.join(data_path, f\"{file_name}_elmo.csv\")\n",
    "if not os.path.isfile(full_path):\n",
    "    # extract ELMo data\n",
    "    elmo.extract(\n",
    "        file_name = file_name,\n",
    "        path = data_path,\n",
    "        batch_size = 20, # lower batch size gives less accurate vectors but requires less memory\n",
    "        doomsday_clock = 50,\n",
    "        confirmation = False\n",
    "    )\n",
    "\n",
    "# load ELMo data\n",
    "print(\"Loading ELMo'd text...\")\n",
    "full_path = os.path.join(data_path, f\"{file_name}_elmo.csv\")\n",
    "elmo_data = pd.read_csv(full_path, header = None)\n",
    "print(f\"ELMo data loaded from {full_path}.\")\n",
    "\n",
    "elmo_df = clean_df.copy()\n",
    "elmo_df = elmo_data.join(elmo_df['category'])\n",
    "elmo_df = elmo_df.dropna()\n",
    "\n",
    "print(f\"Shape of elmo_df: {elmo_df.shape}\")\n",
    "elmo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = elmo_df.shape[1] - 1\n",
    "X = np.asarray(elmo_df.iloc[:, :n])\n",
    "X_2d = PCA(n_components = 2).fit_transform(X)\n",
    "elmo_2d = pd.DataFrame(X_2d, columns = ['x', 'y'])\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize = (15, 10))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "sns.scatterplot(data = elmo_2d, x = 'x', y = 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then perform a one hot encoding for the category variable, as this will make training our model easier. We do this by first creating a dataframe with columns the categories and binary values for every paper, and then concatenate our original dataframe with the binary values."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def cats_to_binary(categories):\n",
    "    '''\n",
    "    Turns categories into a 0-1 sequence with 1's at every category index.\n",
    "    \n",
    "    INPUT\n",
    "        categories, an iterable of strings\n",
    "    \n",
    "    OUTPUT\n",
    "        numpy array with 1 at the category indexes and zeros everywhere else\n",
    "    '''\n",
    "    return np.in1d(cats, categories).astype('int8')\n",
    "\n",
    "print(\"One-hot encoding...\", end = \" \")\n",
    "\n",
    "df_1hot = df_vec.copy()\n",
    "\n",
    "# populate cats_df with the information from df\n",
    "bincat_arr = np.array([cats_to_binary(cat_list) for cat_list in df_1hot['category']]).transpose()\n",
    "bincat_dict = {key:value for (key,value) in zip(cats, bincat_arr)}\n",
    "bincat_df = pd.DataFrame.from_dict(bincat_dict)\n",
    "\n",
    "# concatenate df with the columns in cats_df\n",
    "df_1hot = pd.concat([df_1hot, bincat_df], axis=1, sort=False)\n",
    "\n",
    "# drop the category column\n",
    "df_1hot.drop(['category'], axis=1, inplace=True)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "# show the new columns of the data frame\n",
    "pd.set_option('display.max_colwidth', 10)\n",
    "print(f\"Dimensions of df_1hot: {df_1hot.shape}.\")\n",
    "df_1hot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the categories in our data set are distributed."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# save a dataframe with the amount of papers in each category\n",
    "sum_cats = bincat_df.apply(lambda x: x.sum())\n",
    "\n",
    "# get statistical information about the distribution of the amount of papers\n",
    "sum_cats.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# plot the distribution of the amount of papers in each category\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(x=sum_cats.keys(), height=sum_cats.values)\n",
    "plt.xlabel('Categories', fontsize=13)\n",
    "plt.ylabel('Number of papers', fontsize=13)\n",
    "plt.title('Distribution of categories in data set', fontsize=18)\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our data is not particularly uniformly distributed. These are the categories with the most amount of papers in the data set."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# add the counts to the dataframe and sort \n",
    "cats_df['count'] = sum_cats.values\n",
    "cats_df = cats_df.sort_values(by=['count'], ascending=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "cats_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_cat(cat):\n",
    "    if cat[:8] == 'astro-ph':\n",
    "        agg_cat = 'physics'\n",
    "    elif cat[:2] == 'cs':\n",
    "        agg_cat = 'cs'\n",
    "    elif cat[:5] == 'gr-qc':\n",
    "        agg_cat = 'physics'\n",
    "    elif cat[:3] == 'hep':\n",
    "        agg_cat = 'physics'\n",
    "    elif cat[:4] == 'math':\n",
    "        agg_cat = 'math'\n",
    "    elif cat[:4] == 'nlin':\n",
    "        agg_cat = 'physics'\n",
    "    elif cat[:4] == 'nucl':\n",
    "        agg_cat = 'physics'\n",
    "    elif cat[:7] == 'physics':\n",
    "        agg_cat = 'physics'\n",
    "    elif cat[:8] == 'quant-ph':\n",
    "        agg_cat = 'physics'\n",
    "    elif cat[:4] == 'stat':\n",
    "        agg_cat = 'stats'\n",
    "    else:\n",
    "        agg_cat = 'other'\n",
    "    return agg_cat\n",
    "\n",
    "def aggregate_cats(cats):\n",
    "    return np.asarray([aggregate_cat(cat) for cat in cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_cats_df = cats_df.copy()\n",
    "agg_cats_df['category'] = agg_cats_df['category'].apply(aggregate_cat)\n",
    "agg_cats = np.asarray(agg_cats_df['category'].unique())\n",
    "print(\"Aggregated categories:\")\n",
    "print(agg_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def agg_cats_to_binary(categories):\n",
    "    '''\n",
    "    Turns aggregated categories into a 0-1 sequence with 1's at every category index.\n",
    "    \n",
    "    INPUT\n",
    "        categories, an iterable of strings\n",
    "    \n",
    "    OUTPUT\n",
    "        numpy array with 1 at the category indexes and zeros everywhere else\n",
    "    '''\n",
    "    agg_categories = aggregate_cats(categories)\n",
    "    return np.in1d(agg_cats, agg_categories).astype('int8')\n",
    "\n",
    "print(\"One-hot encoding...\", end = \" \")\n",
    "\n",
    "df_1hot_agg = elmo_df.copy()\n",
    "\n",
    "# populate cats_df with the information from df\n",
    "bincat_arr = np.array([agg_cats_to_binary(cat_list) for cat_list in df_1hot_agg['category']]).transpose()\n",
    "bincat_dict = {key:value for (key,value) in zip(agg_cats, bincat_arr)}\n",
    "bincat_df = pd.DataFrame.from_dict(bincat_dict)\n",
    "\n",
    "# concatenate df with the columns in cats_df\n",
    "df_1hot_agg = pd.concat([df_1hot_agg, bincat_df], axis=1, sort=False)\n",
    "\n",
    "# drop the category column\n",
    "df_1hot_agg.drop(['category'], axis=1, inplace=True)\n",
    "\n",
    "# save the one hot encoded dataframe\n",
    "full_path = os.path.join(data_path, f\"{file_name}_1hot_agg.csv\")\n",
    "df_1hot_agg.to_csv(full_path)\n",
    "\n",
    "print(f\"Done! Also saved the dataframe to {full_path}.\")\n",
    "\n",
    "# load data\n",
    "print(\"Loading aggregated category data...\")\n",
    "full_path = os.path.join(data_path, f\"{file_name}_1hot_agg.csv\")\n",
    "df_1hot_agg = pd.read_csv(full_path, header = None)\n",
    "print(f\"Aggregated category data loaded from {full_path}.\")\n",
    "\n",
    "# show the new columns of the data frame\n",
    "pd.set_option('display.max_colwidth', 10)\n",
    "print(f\"Dimensions of df_1hot_agg: {df_1hot_agg.shape}.\")\n",
    "df_1hot_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a dataframe with the amount of papers in each category\n",
    "sum_cats = bincat_df.apply(lambda x: x.sum())\n",
    "\n",
    "# plot the distribution of the amount of papers in each category\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(x=sum_cats.keys(), height=sum_cats.values)\n",
    "plt.xlabel('Categories', fontsize=13)\n",
    "plt.ylabel('Number of papers', fontsize=13)\n",
    "plt.title('Distribution of categories in data set', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now done manipulating our data, and the time has come to build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = NN.NeuralNetwork(\n",
    "    layer_dims = [1024, 512, 1],\n",
    "    activations = ['tanh', 'tanh', 'sigmoid'],\n",
    "    learning_rate = 0.0075,\n",
    "    num_iterations = 25000,\n",
    "    plot_cost = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(df_1hot_agg.iloc[:, :1024].T)\n",
    "y = np.asarray(df_1hot_agg.loc[:, 'physics'])\n",
    "y = y.reshape(1, y.size)\n",
    "\n",
    "nn_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
