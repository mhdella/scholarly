{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Analysis of Preprint Papers from the ArXiv</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website https://arxiv.org is a popular database for scientific papers in STEM fields. ArXiv has its own classification system consisting of roughly 150 different categories, which are manually added by the authors whenever a new paper is uploaded. A paper can be assigned multiple categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for this project is to develop a machine learning model which can predict the ArXiv category from a given title and abstract. The data set used here has been scraped from the ArXiv API, see https://arxiv.org/help/api, using a Python scraper which can be downloaded from\n",
    "\n",
    "<p><center><a href=https://github.com/saattrupdan/scholarly/blob/master/arxiv_scraper.py>https://github.com/saattrupdan/scholarly/blob/master/arxiv_scraper.py.</a>.</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have downloaded two data sets using this scraper, which can be freely downloaded below:\n",
    "<ul>\n",
    "    <li> The <tt>arxiv.csv</tt> file includes at most 10,000 papers from every ArXiv category. This data set is very large (~1.2gb) and takes a day or two to scrape.</li>\n",
    "    <li> The <tt>arxiv_small.csv</tt> file is more manageable (~200mb) and includes at most 1,000 papers from every ArXiv category.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# download the large data set\n",
    "!wget -O arxiv.csv https://filedn.com/lRBwPhPxgV74tO0rDoe8SpH/arxiv.csv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# download the small data set\n",
    "!wget -O arxiv_small.csv https://filedn.com/lRBwPhPxgV74tO0rDoe8SpH/arxiv_small.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need a list of all the ArXiv categories, which can be downloaded below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# download categories as csv\n",
    "!wget -O cats.csv https://filedn.com/lRBwPhPxgV74tO0rDoe8SpH/cats.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Fetching data </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing all the packages we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re # regular expressions\n",
    "import pickle # enables saving data and models locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first set the file name we're interested in and load up the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the file name of the data set, e.g. 'arxiv' or 'arxiv_small'\n",
    "file_name = 'arxiv_small'\n",
    "\n",
    "# load array of categories\n",
    "cats = np.asarray(pd.read_csv(\"cats.csv\")['category'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we load the data into a dataframe. This takes a while, which is why we are also saving it locally."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# only compile this the first time\n",
    "\n",
    "def nan_if_empty(x):\n",
    "    arr = np.asarray(x)\n",
    "    if arr.size == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return arr\n",
    "\n",
    "def remove_non_cats(x):\n",
    "    arr = np.asarray(x)\n",
    "    return np.intersect1d(arr, cats)\n",
    "\n",
    "def str_to_arr(x):\n",
    "    return np.asarray(re.sub('[\\' \\[\\]]', '', x).split(','))\n",
    "\n",
    "def clean_cats(x):\n",
    "    return nan_if_empty(remove_non_cats(str_to_arr(x)))\n",
    "\n",
    "# set up dataframe, which also converts the category variable from a string to a list\n",
    "df = pd.read_csv(f'{file_name}.csv', converters={'category': clean_cats})[['title', 'abstract', 'category']]\n",
    "\n",
    "# save dataframe to {file_name}_df.pickle\n",
    "with open(f\"{file_name}_df.pickle\",\"wb\") as pickle_out:\n",
    "    pickle.dump(df, pickle_out)\n",
    "\n",
    "print(f\"Dataframe saved to {file_name}_df.pickle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata from 150479 papers. Here are some of them:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92570</th>\n",
       "      <td>Sub-leading asymptotics of ECH capacities</td>\n",
       "      <td>In previous work, the first author and collaborators showed that the leading\\nasymptotics of the embedded contact homology (ECH) spectrum recovers the\\ncontact volume. Our main theorem here is a new bound on the sub-leading\\nasymptotics.</td>\n",
       "      <td>[math.DG, math.SG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40694</th>\n",
       "      <td>Enabling Quality-Driven Scalable Video Transmission over Multi-User NOMA\\n  System</td>\n",
       "      <td>Recently, non-orthogonal multiple access (NOMA) has been proposed to achieve\\nhigher spectral efficiency over conventional orthogonal multiple access.\\nAlthough it has the potential to meet increasing demands of video services, it\\nis still challenging to provide high performance video streami...</td>\n",
       "      <td>[cs.IT, cs.MM, cs.NI, eess.SP, math.IT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82813</th>\n",
       "      <td>Lorentzian length spaces</td>\n",
       "      <td>We introduce an analogue of the theory of length spaces into the setting of\\nLorentzian geometry and causality theory. The r\\^ole of the metric is taken\\nover by the time separation function, in terms of which all basic notions are\\nformulated. In this way we recover many fundamental results i...</td>\n",
       "      <td>[gr-qc, math-ph, math.DG, math.MG, math.MP]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    title  \\\n",
       "92570                                           Sub-leading asymptotics of ECH capacities   \n",
       "40694  Enabling Quality-Driven Scalable Video Transmission over Multi-User NOMA\\n  System   \n",
       "82813                                                            Lorentzian length spaces   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                          abstract  \\\n",
       "92570                                                                In previous work, the first author and collaborators showed that the leading\\nasymptotics of the embedded contact homology (ECH) spectrum recovers the\\ncontact volume. Our main theorem here is a new bound on the sub-leading\\nasymptotics.   \n",
       "40694    Recently, non-orthogonal multiple access (NOMA) has been proposed to achieve\\nhigher spectral efficiency over conventional orthogonal multiple access.\\nAlthough it has the potential to meet increasing demands of video services, it\\nis still challenging to provide high performance video streami...   \n",
       "82813    We introduce an analogue of the theory of length spaces into the setting of\\nLorentzian geometry and causality theory. The r\\^ole of the metric is taken\\nover by the time separation function, in terms of which all basic notions are\\nformulated. In this way we recover many fundamental results i...   \n",
       "\n",
       "                                          category  \n",
       "92570                           [math.DG, math.SG]  \n",
       "40694      [cs.IT, cs.MM, cs.NI, eess.SP, math.IT]  \n",
       "82813  [gr-qc, math-ph, math.DG, math.MG, math.MP]  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataframe    \n",
    "with open(f\"{file_name}_df.pickle\", \"rb\") as pickle_in:\n",
    "    df = pickle.load(pickle_in)\n",
    "\n",
    "print(f\"Loaded metadata from {df.shape[0]} papers. Here are some of them:\")\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Cleaning the data </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next do some basic cleaning of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>141086</th>\n",
       "      <td>Visualizing Treasury Issuance Strategy</td>\n",
       "      <td>We introduce simple cost and risk proxy metrics that can be attached to\\nTreasury issuance strategy to complement analysis of the resulting portfolio\\nweighted-average maturity (WAM). These metrics are based on mapping issuance\\nfractions to their long-term, asymptotic portfolio implications f...</td>\n",
       "      <td>visualizing treasury issuance strategy we introduce simple cost and risk proxy metrics that can be attached to treasury issuance strategy to complement analysis of the resulting portfolio weighted-average maturity (wam). these metrics are based on mapping issuance fractions to their long-term, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144991</th>\n",
       "      <td>Inferring the temporal structure of directed functional connectivity in\\n  neural systems: some extensions to Granger causality</td>\n",
       "      <td>Neural processes in the brain operate at a range of temporal scales. Granger\\ncausality, the most widely-used neuroscientific tool for inference of directed\\nfunctional connectivity from neurophsyiological data, is traditionally deployed\\nin the form of one-step-ahead prediction regardless of ...</td>\n",
       "      <td>inferring the temporal structure of directed functional connectivity in neural systems: some extensions to granger causality neural processes in the brain operate at a range of temporal scales. granger causality, the most widely-used neuroscientific tool for inference of directed functional conn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124939</th>\n",
       "      <td>Modeling observations of solar coronal mass ejections with heliospheric\\n  imagers verified with the Heliophysics System Observatory</td>\n",
       "      <td>We present an advance towards accurately predicting the arrivals of coronal\\nmass ejections (CMEs) at the terrestrial planets, including Earth. For the\\nfirst time, we are able to assess a CME prediction model using data over 2/3 of\\na solar cycle of observations with the Heliophysics System O...</td>\n",
       "      <td>modeling observations of solar coronal mass ejections with heliospheric imagers verified with the heliophysics system observatory we present an advance towards accurately predicting the arrivals of coronal mass ejections (cmes) at the terrestrial planets, including earth. for the first time, we ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                       title  \\\n",
       "141086                                                                                                Visualizing Treasury Issuance Strategy   \n",
       "144991       Inferring the temporal structure of directed functional connectivity in\\n  neural systems: some extensions to Granger causality   \n",
       "124939  Modeling observations of solar coronal mass ejections with heliospheric\\n  imagers verified with the Heliophysics System Observatory   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                           abstract  \\\n",
       "141086    We introduce simple cost and risk proxy metrics that can be attached to\\nTreasury issuance strategy to complement analysis of the resulting portfolio\\nweighted-average maturity (WAM). These metrics are based on mapping issuance\\nfractions to their long-term, asymptotic portfolio implications f...   \n",
       "144991    Neural processes in the brain operate at a range of temporal scales. Granger\\ncausality, the most widely-used neuroscientific tool for inference of directed\\nfunctional connectivity from neurophsyiological data, is traditionally deployed\\nin the form of one-step-ahead prediction regardless of ...   \n",
       "124939    We present an advance towards accurately predicting the arrivals of coronal\\nmass ejections (CMEs) at the terrestrial planets, including Earth. For the\\nfirst time, we are able to assess a CME prediction model using data over 2/3 of\\na solar cycle of observations with the Heliophysics System O...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                         clean_text  \n",
       "141086  visualizing treasury issuance strategy we introduce simple cost and risk proxy metrics that can be attached to treasury issuance strategy to complement analysis of the resulting portfolio weighted-average maturity (wam). these metrics are based on mapping issuance fractions to their long-term, a...  \n",
       "144991  inferring the temporal structure of directed functional connectivity in neural systems: some extensions to granger causality neural processes in the brain operate at a range of temporal scales. granger causality, the most widely-used neuroscientific tool for inference of directed functional conn...  \n",
       "124939  modeling observations of solar coronal mass ejections with heliospheric imagers verified with the heliophysics system observatory we present an advance towards accurately predicting the arrivals of coronal mass ejections (cmes) at the terrestrial planets, including earth. for the first time, we ...  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows with NaNs\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# merge title and abstract\n",
    "df['clean_text'] = df['title'] + ' ' + df['abstract']\n",
    "\n",
    "# remove punctuation marks\n",
    "punctuation ='\\!\\\"\\#\\$\\%\\&\\(\\)\\*\\+\\-\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~'\n",
    "df['clean_text'] = df['clean_text'].apply(lambda x: re.sub(punctuation, '', x))\n",
    "\n",
    "# convert text to lowercase\n",
    "df['clean_text'] = df['clean_text'].str.lower()\n",
    "\n",
    "# remove numbers\n",
    "df['clean_text'] = df['clean_text'].str.replace(\"[0-9]\", \" \")\n",
    "\n",
    "# remove whitespaces\n",
    "df['clean_text'] = df['clean_text'].apply(lambda x:' '.join(x.split()))\n",
    "\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "df[['title', 'abstract', 'clean_text']].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last text cleaning step is to lemmatise the text, which reduces all words to its base form. For instance, 'eating' is converted into 'eat' and 'better' is converted into 'good'. This usually takes a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download spacy if needed\n",
    "#!pip install -U spacy && python -m spacy download en\n",
    "\n",
    "from spacy import load as sp\n",
    "\n",
    "def lemmatization(texts):\n",
    "\n",
    "    # import spaCy's language model\n",
    "    nlp = sp('en', disable=['parser', 'ner'])\n",
    "    \n",
    "    output = []\n",
    "    for text in texts:\n",
    "        s = [token.lemma_ for token in nlp(text)]\n",
    "        output.append(' '.join(s))\n",
    "    \n",
    "    return output\n",
    "\n",
    "# lemmatise text\n",
    "df['clean_text'] = lemmatization(df['clean_text'])\n",
    "\n",
    "# save dataframe to {file_name}_clean_text.pickle\n",
    "with open(f\"{file_name}_clean_text.pickle\",\"wb\") as pickle_out:\n",
    "    pickle.dump(df[['clean_text']], pickle_out)\n",
    "\n",
    "print(f\"Lemmatisation complete. Dataframe with the clean_text column saved to {file_name}_clean_text.pickle.\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> One hot encoding of categories </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then perform a one hot encoding for the category variable, as this will make training our model easier. We do this by first creating a dataframe with columns the categories and binary values for every paper, and then concatenate our original dataframe with the binary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cat_to_binary(x):\n",
    "    cat_index = np.nonzero(cats == x)[0][0]\n",
    "    return [0] * cat_index + [1] + [0] * (len(cats) - cat_index - 1)\n",
    "\n",
    "def cats_to_binary(x):\n",
    "    binary_cat = np.sum([cat_to_binary(y) for y in x], 0, dtype=np.int32)\n",
    "    return binary_cat.tolist()\n",
    "\n",
    "# populate cats_df with the information from df\n",
    "bincat_list = np.array([cats_to_binary(x) for x in df['category']]).transpose().tolist()\n",
    "bincat_dict = {key:value for (key,value) in list(zip(cats, cat_list))}\n",
    "bincat_df = pd.DataFrame.from_dict(cat_dict)\n",
    "\n",
    "# concatenate df with the columns in cats_df\n",
    "df = pd.concat([df, bincat_df], axis=1, sort=False)\n",
    "\n",
    "# drop the category column\n",
    "df.drop(['category'], axis=1, inplace=True)\n",
    "\n",
    "print(\"One hot encoding complete.\")\n",
    "\n",
    "# show the new columns of the data frame\n",
    "pd.set_option('display.max_colwidth', 10)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Analysis of the data </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the categories in our data set are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find the amount of papers in each category\n",
    "sum_cats = bincat_df.apply(lambda x: x.sum())\n",
    "sum_cats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the distribution of the amount of papers in each category\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(x=sum_cats.keys(), height=sum_cats.values)\n",
    "plt.xlabel('Categories', fontsize=13)\n",
    "plt.ylabel('Number of papers', fontsize=13)\n",
    "plt.title('Distribution of categories in data set', fontsize=18)\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = pd.read_csv(\"cats.csv\")\n",
    "cat_df['count'] = sum_cats.values\n",
    "cat_df = cat_df.sort_values(by=['count'], ascending=False)\n",
    "\n",
    "cat_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> ELMo feature extraction </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our model we have to extract features from the titles and abstracts. We will be using ELMo, a state-of-the-art NLP framework developed by AllenNLP, which converts text input into vectors, with similar words being closer to each other. We will need the following extra packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download the ELMo model. It is over 350mb in size, so it might take a little while."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# download the ELMo model, only compile this the first time\n",
    "!mkdir elmo\n",
    "!curl -L \"https://tfhub.dev/google/elmo/2?tf-hub-format=compressed\" | tar -zxvC elmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to extract ELMo features from our cleaned text data. This takes a LONG time (potentially many days) and will usually crash if run in a notebook. Instead this is done using the Python extractor which can be downloaded from\n",
    "\n",
    "<p><center><a href=\"\n",
    "\n",
    "\n",
    "to the <tt>extract_elmo.py</tt>, which is run with the data file as input without its file extension. For instance, the following extracts the features from the <tt>arxiv.csv</tt> file and saves the features as <tt>arxiv_elmo.pickle</tt>:\n",
    "<p><center><tt>$ python extract_elmo.py arxiv</tt></center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the ELMo features have now been extracted, we now load them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo data loaded!\n"
     ]
    }
   ],
   "source": [
    "# load ELMo data\n",
    "with open(f\"{file_name}_elmo.pickle\", \"rb\") as pickle_in:\n",
    "    elmo_data = pickle.load(pickle_in)\n",
    "\n",
    "print(f\"ELMo data loaded from {file_name}_elmo.pickle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Building the model </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now done manipulating our data, and the time has come to build a model. We will be building a logistic regression model for every category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = []\n",
    "Y_hat = []\n",
    "lregs = np.asarray([])\n",
    "\n",
    "for cat in cats:\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(elmo_data, df[cat], random_state=4, test_size=0.2)\n",
    "    Y_test.append(y_test)\n",
    "    \n",
    "    try:\n",
    "        lreg = LogisticRegression(C=0.0001, solver='liblinear')\n",
    "        lreg.fit(X_train, y_train)\n",
    "        Y_hat.append(lreg.predict(X_test))\n",
    "        lregs.append(lreg)\n",
    "    except:\n",
    "        Y_hat.append(np.array([0] * len(y_test)))\n",
    "\n",
    "with open(f\"{file_name}_model.pickle\",\"wb\") as pickle_out:\n",
    "    pickle.dump(lregs, pickle_out)\n",
    "        \n",
    "print(f\"Model built and saved to {file_name}_model.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Testing the model </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check whether our model is capable of predicting the categories for the papers in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest f1 score for the model is 0.8571428571428571.\n",
      "The average f1 score for the model is 0.0056022408963585435.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leidem/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "f1s = [f1_score(x, y) for (x,y) in zip(Y_test, Y_hat)]\n",
    "\n",
    "print(f\"The average f1 score for the model is {mean(f1s)}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
